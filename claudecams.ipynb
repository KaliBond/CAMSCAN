{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt48XLquwVPvKygbgAj8cP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaliBond/CAMSCAN/blob/main/claudecams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7BN8ToTZsTCG",
        "outputId": "0075f6ef-40c2-4321-9136-06003f2de0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üß† CAMS-CAN v4.0 - Enhanced Complex Adaptive Analysis\n",
            "============================================================\n",
            "============================================================\n",
            "üìÅ Please upload your CAMS CSV file.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4e8e7372-896d-4bcd-a8ff-78ea919f6fe9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4e8e7372-896d-4bcd-a8ff-78ea919f6fe9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-2262579481.py\", line 973, in <cell line: 0>\n",
            "    df, network, dashboard, insights = run_enhanced_cams_analysis()\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2262579481.py\", line 892, in run_enhanced_cams_analysis\n",
            "    df = load_and_validate_data()\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2262579481.py\", line 325, in load_and_validate_data\n",
            "    uploaded = files.upload()\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/files.py\", line 72, in upload\n",
            "    uploaded_files = _upload_files(multiple=True)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/files.py\", line 164, in _upload_files\n",
            "    result = _output.eval_js(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\", line 40, in eval_js\n",
            "    return _message.read_reply_from_input(request_id, timeout_sec)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\", line 96, in read_reply_from_input\n",
            "    time.sleep(0.025)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 969, in getmodule\n",
            "    def getmodule(object, _filename=None):\n",
            "    \n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2262579481.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdashboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_enhanced_cams_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2262579481.py\u001b[0m in \u001b[0;36mrun_enhanced_cams_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m         \u001b[0msociety_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Society'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2262579481.py\u001b[0m in \u001b[0;36mload_and_validate_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "# Enhanced CAMS-CAN v4.0 - Complete Implementation\n",
        "# Fully implements the mathematical formulation with all dynamics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "from google.colab import files\n",
        "import io\n",
        "from scipy.integrate import odeint\n",
        "from scipy.stats import entropy\n",
        "import warnings\n",
        "import json # Import json for saving insights\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============= CORE CAMS MATHEMATICAL MODEL =============\n",
        "\n",
        "class CAMSNode:\n",
        "    \"\"\"Represents a single node in the CAMS network\"\"\"\n",
        "    def __init__(self, node_id, coherence, capacity, stress, abstraction, memory=0):\n",
        "        self.id = node_id\n",
        "        self.coherence = coherence\n",
        "        self.capacity = capacity\n",
        "        self.stress_chronic = stress * 0.7  # Decompose stress\n",
        "        self.stress_acute = stress * 0.3\n",
        "        self.abstraction = abstraction\n",
        "        self.memory = memory\n",
        "        self.threshold = 5.0  # Base threshold Œ∏_i0\n",
        "\n",
        "    def calculate_effective_threshold(self):\n",
        "        \"\"\"Œ∏_i(t) = Œ∏_i0 + Œ∑_œÉ * œÉ_i(t) - Œ∑_œá * œá_i(t)\"\"\"\n",
        "        eta_sigma = 0.3  # stress raises threshold\n",
        "        eta_chi = 0.2    # coherence lowers threshold\n",
        "        total_stress = self.stress_chronic + self.stress_acute\n",
        "        return self.threshold + eta_sigma * total_stress - eta_chi * self.coherence\n",
        "\n",
        "    def neuromodulated_input(self, weights, inputs, beta=0.5, alpha_acute=2.0):\n",
        "        \"\"\"u_i(t) = Œ£ w_ij * s_j - Œ∏_i + Œ≤_i(œÉ_ch + Œ±_acute * œÉ_ac)\"\"\"\n",
        "        weighted_sum = np.sum(weights * inputs)\n",
        "        threshold = self.calculate_effective_threshold()\n",
        "        stress_modulation = beta * (self.stress_chronic + alpha_acute * self.stress_acute)\n",
        "        return weighted_sum - threshold + stress_modulation\n",
        "\n",
        "    def activation(self, u):\n",
        "        \"\"\"a_i(t) = œÉ_act(u_i(t)) using sigmoid\"\"\"\n",
        "        return 1 / (1 + np.exp(-u))\n",
        "\n",
        "class CAMSNetwork:\n",
        "    \"\"\"Complete CAMS network implementation with full dynamics\"\"\"\n",
        "\n",
        "    def __init__(self, nodes_data):\n",
        "        self.nodes = {}\n",
        "        self.bond_matrix = None\n",
        "        self.weight_matrix = None\n",
        "        self.time = 0\n",
        "        self.dt = 0.1\n",
        "\n",
        "        # Initialize nodes\n",
        "        for node_data in nodes_data:\n",
        "            node = CAMSNode(\n",
        "                node_data['Node'],\n",
        "                node_data['Coherence'],\n",
        "                node_data['Capacity'],\n",
        "                node_data['Stress'],\n",
        "                node_data['Abstraction']\n",
        "            )\n",
        "            self.nodes[node.id] = node\n",
        "\n",
        "        # Initialize matrices\n",
        "        self.initialize_networks()\n",
        "\n",
        "    def initialize_networks(self):\n",
        "        \"\"\"Initialize bond and weight matrices\"\"\"\n",
        "        n = len(self.nodes)\n",
        "        node_ids = list(self.nodes.keys())\n",
        "\n",
        "        # Bond matrix B_ij\n",
        "        self.bond_matrix = np.ones((n, n)) * 0.5\n",
        "        np.fill_diagonal(self.bond_matrix, 0)\n",
        "\n",
        "        # Weight matrix w_ij\n",
        "        self.weight_matrix = np.random.randn(n, n) * 0.1\n",
        "        np.fill_diagonal(self.weight_matrix, 0)\n",
        "\n",
        "        # Node mapping\n",
        "        self.node_to_idx = {node_id: i for i, node_id in enumerate(node_ids)}\n",
        "        self.idx_to_node = {i: node_id for i, node_id in enumerate(node_ids)}\n",
        "\n",
        "    def update_capacity(self, node, r_kappa=0.1, phi_kappa=0.05, xi_kappa=0.2, zeta_kappa=0.1):\n",
        "        \"\"\"dŒ∫_i/dt = r_Œ∫ * a_i + œÜ_Œ∫ * Œ£ B_ij(Œ∫_j - Œ∫_i) - Œæ_Œ∫ * œÉ_i - Œ∂_Œ∫ * C(t)\"\"\"\n",
        "        idx = self.node_to_idx[node.id]\n",
        "\n",
        "        # Calculate activation\n",
        "        inputs = np.array([self.nodes[self.idx_to_node[j]].capacity\n",
        "                          for j in range(len(self.nodes))])\n",
        "        u = node.neuromodulated_input(self.weight_matrix[idx], inputs)\n",
        "        a_i = node.activation(u)\n",
        "\n",
        "        # Bond diffusion term\n",
        "        bond_diffusion = 0\n",
        "        for j, other_id in enumerate(self.idx_to_node.values()):\n",
        "            if other_id != node.id:\n",
        "                other = self.nodes[other_id]\n",
        "                bond_diffusion += self.bond_matrix[idx, j] * (other.capacity - node.capacity)\n",
        "\n",
        "        # Complexity load\n",
        "        complexity = np.sum(np.abs(self.bond_matrix))\n",
        "\n",
        "        # Total stress\n",
        "        total_stress = node.stress_chronic + node.stress_acute\n",
        "\n",
        "        # Update equation\n",
        "        dkappa_dt = (r_kappa * a_i +\n",
        "                    phi_kappa * bond_diffusion -\n",
        "                    xi_kappa * total_stress -\n",
        "                    zeta_kappa * complexity)\n",
        "\n",
        "        return dkappa_dt\n",
        "\n",
        "    def update_coherence(self, node, lambda_chi=0.05, mu_chi=0.1, rho_chi=0.15):\n",
        "        \"\"\"dœá_i/dt = -Œª_œá * œá_i + Œº_œá * maint_i - Œ£ B_ij(œá_i - œá_j) - œÅ_œá * œÉ_i * œá_i\"\"\"\n",
        "        idx = self.node_to_idx[node.id]\n",
        "\n",
        "        # Maintenance (simplified as capacity-based)\n",
        "        maintenance = node.capacity * 0.1\n",
        "\n",
        "        # Bond diffusion\n",
        "        bond_diffusion = 0\n",
        "        for j, other_id in enumerate(self.idx_to_node.values()):\n",
        "            if other_id != node.id:\n",
        "                other = self.nodes[other_id]\n",
        "                bond_diffusion += self.bond_matrix[idx, j] * (node.coherence - other.coherence)\n",
        "\n",
        "        # Total stress\n",
        "        total_stress = node.stress_chronic + node.stress_acute\n",
        "\n",
        "        # Update equation\n",
        "        dchi_dt = (-lambda_chi * node.coherence +\n",
        "                   mu_chi * maintenance -\n",
        "                   bond_diffusion -\n",
        "                   rho_chi * total_stress * node.coherence)\n",
        "\n",
        "        return dchi_dt\n",
        "\n",
        "    def update_stress(self, node, lambda_sigma=0.1, rho_sigma=0.2, omega_sigma=0.1, upsilon_sigma=0.05):\n",
        "        \"\"\"dœÉ_i/dt = -Œª_œÉ * œÉ_i + œÅ_œÉ * Œ£ B_ij(œÉ_j - œÉ_i) + Œµ_i + œÖ_œÉ * M_i - œâ_œÉ * a_i\"\"\"\n",
        "        idx = self.node_to_idx[node.id]\n",
        "\n",
        "        # Calculate activation\n",
        "        inputs = np.array([self.nodes[self.idx_to_node[j]].capacity\n",
        "                          for j in range(len(self.nodes))])\n",
        "        u = node.neuromodulated_input(self.weight_matrix[idx], inputs)\n",
        "        a_i = node.activation(u)\n",
        "\n",
        "        # Stress diffusion\n",
        "        stress_diffusion = 0\n",
        "        for j, other_id in enumerate(self.idx_to_node.values()):\n",
        "            if other_id != node.id:\n",
        "                other = self.nodes[other_id]\n",
        "                total_stress_j = other.stress_chronic + other.stress_acute\n",
        "                total_stress_i = node.stress_chronic + node.stress_acute\n",
        "                stress_diffusion += self.bond_matrix[idx, j] * (total_stress_j - total_stress_i)\n",
        "\n",
        "        # Memory load\n",
        "        memory_load = node.memory * upsilon_sigma\n",
        "\n",
        "        # Random shock (small)\n",
        "        shock = np.random.normal(0, 0.01)\n",
        "\n",
        "        # Update equation\n",
        "        total_stress = node.stress_chronic + node.stress_acute\n",
        "        dsigma_dt = (-lambda_sigma * total_stress +\n",
        "                     rho_sigma * stress_diffusion +\n",
        "                     shock +\n",
        "                     memory_load -\n",
        "                     omega_sigma * a_i)\n",
        "\n",
        "        return dsigma_dt\n",
        "\n",
        "    def update_abstraction(self, node, gamma_alpha=0.05, delta_alpha=0.1):\n",
        "        \"\"\"dŒ±_i/dt = Œ≥_Œ± * M_i - Œ¥_Œ± * œÉ_i\"\"\"\n",
        "        total_stress = node.stress_chronic + node.stress_acute\n",
        "        dalpha_dt = gamma_alpha * node.memory - delta_alpha * total_stress\n",
        "        return dalpha_dt\n",
        "\n",
        "    def update_memory(self, node):\n",
        "        \"\"\"M_i(t+dt) = M_i(t) + dt * Œ±_i * œá_i\"\"\"\n",
        "        return node.abstraction * node.coherence * self.dt\n",
        "\n",
        "    def calculate_system_health(self):\n",
        "        \"\"\"H(t) = Œ£ w_i * h_i(t) * (1 - P_t)\"\"\"\n",
        "        health_sum = 0\n",
        "        total_stress = 0\n",
        "        coherence_values = []\n",
        "\n",
        "        for node in self.nodes.values():\n",
        "            # Calculate denominator D_i\n",
        "            idx = self.node_to_idx[node.id]\n",
        "            bond_sum = np.sum(self.bond_matrix[idx])\n",
        "            stress_weighted = node.stress_chronic + 2 * node.stress_acute\n",
        "            D_i = (1 + bond_sum) * (stress_weighted + 0.01)  # Avoid division by zero\n",
        "\n",
        "            # Base node output h_i\n",
        "            h_i = (node.coherence * node.capacity * (1 + 0.5 * node.abstraction)) / D_i\n",
        "\n",
        "            # Weight (use bond strength as proxy)\n",
        "            weight = np.mean(self.bond_matrix[idx])\n",
        "            health_sum += weight * h_i\n",
        "\n",
        "            total_stress += node.stress_chronic + node.stress_acute\n",
        "            coherence_values.append(node.coherence)\n",
        "\n",
        "        # Calculate penalty term (Coherence Asymmetry)\n",
        "        ca = np.std(coherence_values) / (np.mean(coherence_values) + 0.01)\n",
        "        stress_tilt = total_stress / (sum(coherence_values) + 0.01)\n",
        "        P_t = min(ca * stress_tilt, 0.75)\n",
        "\n",
        "        return health_sum * (1 - P_t)\n",
        "\n",
        "    def calculate_coherence_entropy(self):\n",
        "        \"\"\"E_œá(t) = -Œ£ œá_i * log(œá_i)\"\"\"\n",
        "        coherence_values = np.array([node.coherence for node in self.nodes.values()])\n",
        "        # Normalize to probabilities\n",
        "        coherence_probs = coherence_values / (np.sum(coherence_values) + 0.01)\n",
        "        coherence_probs = np.clip(coherence_probs, 1e-10, 1)  # Avoid log(0)\n",
        "        return -np.sum(coherence_probs * np.log(coherence_probs))\n",
        "\n",
        "    def calculate_network_synchronization(self):\n",
        "        \"\"\"S_sync(t) = 1/n¬≤ * Œ£ |dN_i/dt - dN_j/dt|\"\"\"\n",
        "        derivatives = []\n",
        "        for node in self.nodes.values():\n",
        "            # Simplified: use capacity change as proxy for dN/dt\n",
        "            dN_dt = self.update_capacity(node)\n",
        "            derivatives.append(dN_dt)\n",
        "\n",
        "        n = len(derivatives)\n",
        "        sync_sum = 0\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                sync_sum += abs(derivatives[i] - derivatives[j])\n",
        "\n",
        "        return sync_sum / (n * n)\n",
        "\n",
        "    def calculate_stress_variance(self):\n",
        "        \"\"\"V_œÉ(t) = 1/n * Œ£ (œÉ_i - œÉ_mean)¬≤\"\"\"\n",
        "        stress_values = [node.stress_chronic + node.stress_acute\n",
        "                        for node in self.nodes.values()]\n",
        "        return np.var(stress_values)\n",
        "\n",
        "    def calculate_legitimacy(self, beta1=0.4, beta2=0.3, beta3=0.3):\n",
        "        \"\"\"L(t) = Œ≤_1 * H(t) + Œ≤_2 * I(t) + Œ≤_3 * œá_mean\"\"\"\n",
        "        health = self.calculate_system_health()\n",
        "\n",
        "        # Information integration I(t) = Œ∑_I * Œ£ Œ±_i * Œ∫_i\n",
        "        info = sum(node.abstraction * node.capacity for node in self.nodes.values()) * 0.1\n",
        "\n",
        "        # Mean coherence\n",
        "        chi_mean = np.mean([node.coherence for node in self.nodes.values()])\n",
        "\n",
        "        return beta1 * health + beta2 * info + beta3 * chi_mean\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Perform one time step of the simulation\"\"\"\n",
        "        # Store updates to apply simultaneously\n",
        "        updates = {}\n",
        "\n",
        "        for node_id, node in self.nodes.items():\n",
        "            updates[node_id] = {\n",
        "                'capacity': self.update_capacity(node),\n",
        "                'coherence': self.update_coherence(node),\n",
        "                'stress': self.update_stress(node),\n",
        "                'abstraction': self.update_abstraction(node),\n",
        "                'memory': self.update_memory(node)\n",
        "            }\n",
        "\n",
        "        # Apply updates\n",
        "        for node_id, node_updates in updates.items():\n",
        "            node = self.nodes[node_id]\n",
        "            node.capacity += node_updates['capacity'] * self.dt\n",
        "            node.coherence += node_updates['coherence'] * self.dt\n",
        "\n",
        "            # Update stress (split between chronic and acute)\n",
        "            stress_change = node_updates['stress'] * self.dt\n",
        "            node.stress_chronic += stress_change * 0.7\n",
        "            node.stress_acute += stress_change * 0.3\n",
        "\n",
        "            node.abstraction += node_updates['abstraction'] * self.dt\n",
        "            node.memory += node_updates['memory']\n",
        "\n",
        "            # Bound values\n",
        "            node.capacity = np.clip(node.capacity, 0, 10)\n",
        "            node.coherence = np.clip(node.coherence, 0, 10)\n",
        "            node.stress_chronic = np.clip(node.stress_chronic, 0, 10)\n",
        "            node.stress_acute = np.clip(node.stress_acute, 0, 10)\n",
        "            node.abstraction = np.clip(node.abstraction, 0, 10)\n",
        "\n",
        "        self.time += self.dt\n",
        "\n",
        "    def simulate(self, steps=100):\n",
        "        \"\"\"Run simulation for multiple steps\"\"\"\n",
        "        history = []\n",
        "        for _ in range(steps):\n",
        "            self.step()\n",
        "            state = {\n",
        "                'time': self.time,\n",
        "                'health': self.calculate_system_health(),\n",
        "                'entropy': self.calculate_coherence_entropy(),\n",
        "                'synchronization': self.calculate_network_synchronization(),\n",
        "                'stress_variance': self.calculate_stress_variance(),\n",
        "                'legitimacy': self.calculate_legitimacy()\n",
        "            }\n",
        "            history.append(state)\n",
        "        return history\n",
        "\n",
        "# ============= DATA PROCESSING & ANALYSIS =============\n",
        "\n",
        "def load_and_validate_data():\n",
        "    \"\"\"Load CSV data with validation\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üìÅ Please upload your CAMS CSV file.\")\n",
        "    print(\"=\" * 60)\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No file uploaded\")\n",
        "\n",
        "    df = pd.read_csv(io.BytesIO(list(uploaded.values())[0]))\n",
        "\n",
        "    # Validate columns\n",
        "    required = ['Society', 'Year', 'Node', 'Coherence', 'Capacity', 'Stress', 'Abstraction']\n",
        "    missing = [col for col in required if col not in df.columns]\n",
        "    if missing:\n",
        "        # Try alternative column names\n",
        "        if 'Nation' in df.columns and 'Society' not in df.columns:\n",
        "            df['Society'] = df['Nation']\n",
        "        else:\n",
        "            raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
        "\n",
        "\n",
        "    # Clean data\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Calculate Node Value and Bond Strength if missing\n",
        "    if 'Node Value' not in df.columns:\n",
        "        df['Node Value'] = df['Coherence'] + df['Capacity'] - df['Stress'] + 0.5 * df['Abstraction']\n",
        "\n",
        "    if 'Bond Strength' not in df.columns:\n",
        "        # Simplified bond strength calculation\n",
        "        df['Bond Strength'] = ((df['Coherence'] + df['Capacity']) * 0.6 +\n",
        "                               df['Abstraction'] * 0.4) / (1 + np.abs(df['Stress']))\n",
        "\n",
        "    print(f\"‚úÖ Loaded data for {df['Society'].iloc[0]}\")\n",
        "    print(f\"üìä Years: {df['Year'].min()}-{df['Year'].max()}\")\n",
        "    print(f\"üîó Nodes: {len(df['Node'].unique())} unique nodes\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_civilization_type(df):\n",
        "    \"\"\"Classify civilization type based on CAMS patterns\"\"\"\n",
        "    # Calculate average metrics\n",
        "    avg_abstraction = df['Abstraction'].mean()\n",
        "    avg_capacity = df['Capacity'].mean()\n",
        "    avg_stress = df['Stress'].abs().mean()\n",
        "    avg_coherence = df['Coherence'].mean()\n",
        "    stress_volatility = df.groupby('Year')['Stress'].std().mean()\n",
        "\n",
        "    # Classification logic\n",
        "    if avg_abstraction > 7 and avg_capacity > 7:\n",
        "        civ_type = \"Supergiant\"\n",
        "        description = \"High abstraction and capacity, burning bright but potentially unstable\"\n",
        "        color = \"#FF6B6B\"\n",
        "    elif avg_coherence > 7 and stress_volatility < 1.5:\n",
        "        civ_type = \"Stable Core\"\n",
        "        description = \"Long-lived, highly coherent, conservatively adaptive\"\n",
        "        color = \"#4ECDC4\"\n",
        "    elif avg_stress > 6:\n",
        "        civ_type = \"Fragile High-Stress\"\n",
        "        description = \"High pressure system, prone to collapse\"\n",
        "        color = \"#FF8C42\"\n",
        "    else:\n",
        "        civ_type = \"Resilient Frontier\"\n",
        "        description = \"Adaptable, quick to respond to changes\"\n",
        "        color = \"#95E77E\"\n",
        "\n",
        "    return {\n",
        "        'type': civ_type,\n",
        "        'description': description,\n",
        "        'color': color,\n",
        "        'metrics': {\n",
        "            'avg_abstraction': avg_abstraction,\n",
        "            'avg_capacity': avg_capacity,\n",
        "            'avg_stress': avg_stress,\n",
        "            'avg_coherence': avg_coherence,\n",
        "            'stress_volatility': stress_volatility\n",
        "        }\n",
        "    }\n",
        "\n",
        "def create_advanced_dashboard(df, network_history=None):\n",
        "    \"\"\"Create comprehensive 15-panel dashboard\"\"\"\n",
        "    society = df['Society'].iloc[0]\n",
        "    years = sorted(df['Year'].unique())\n",
        "\n",
        "    # Analyze civilization type\n",
        "    civ_analysis = analyze_civilization_type(df)\n",
        "\n",
        "    # Create figure\n",
        "    fig = make_subplots(\n",
        "        rows=5, cols=3,\n",
        "        subplot_titles=(\n",
        "            f\"{civ_analysis['type']}: System Health\", \"Stress Decomposition\", \"Network Topology\",\n",
        "            \"Coherence Entropy\", \"Phase Space\", \"Bond Matrix Heatmap\",\n",
        "            \"Early Warning Signals\", \"Capacity Evolution\", \"Abstraction Dynamics\",\n",
        "            \"Stress Propagation\", \"Legitimacy Index\", \"Path Dependence\",\n",
        "            \"Comparative Resilience\", \"System Status Indicator\", \"Future Projections\" # Corrected Panel 14 title\n",
        "        ),\n",
        "        specs=[\n",
        "            [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"type\": \"scatter3d\"}],\n",
        "            [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"type\": \"heatmap\"}],\n",
        "            [{\"secondary_y\": True}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "            [{\"secondary_y\": False}, {\"type\": \"heatmap\"}, {\"secondary_y\": False}], # Corrected Panel 10 type\n",
        "            [{\"type\": \"bar\"}, {\"type\": \"indicator\"}, {\"secondary_y\": False}] # Corrected Panel 13 type\n",
        "        ],\n",
        "        vertical_spacing=0.08,\n",
        "        horizontal_spacing=0.1,\n",
        "        row_heights=[0.2, 0.2, 0.2, 0.2, 0.2] # Added row heights for better spacing\n",
        "    )\n",
        "\n",
        "    # Panel 1: System Health with civilization type\n",
        "    health_by_year = df.groupby('Year').apply(\n",
        "        lambda x: (x['Coherence'] * x['Capacity']).sum() / (1 + x['Stress'].abs().sum())\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=health_by_year, mode='lines+markers',\n",
        "                  name='System Health', line=dict(color=civ_analysis['color'], width=3)),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    fig.update_yaxes(title_text=\"System Health\", row=1, col=1)\n",
        "\n",
        "\n",
        "    # Panel 2: Stress Decomposition (Chronic vs Acute)\n",
        "    stress_chronic_mean = df.groupby('Year')['Stress'].mean() * 0.7\n",
        "    stress_acute_std = df.groupby('Year')['Stress'].std() * 0.3 # Using std as a proxy for acute stress fluctuation\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=stress_chronic_mean, name='Chronic Stress (Mean)',\n",
        "                  line=dict(color='#8B4513')), row=1, col=2\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=stress_acute_std, name='Acute Stress (Std Dev)', # Clarified label\n",
        "                  line=dict(color='#FF4500')), row=1, col=2\n",
        "    )\n",
        "    fig.update_yaxes(title_text=\"Stress Level\", row=1, col=2)\n",
        "\n",
        "\n",
        "    # Panel 3: 3D Network Topology\n",
        "    latest_year = years[-1]\n",
        "    latest_data = df[df['Year'] == latest_year]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x=latest_data['Coherence'],\n",
        "            y=latest_data['Capacity'],\n",
        "            z=latest_data['Abstraction'],\n",
        "            mode='markers+text',\n",
        "            marker=dict(size=latest_data['Node Value']*2,\n",
        "                       color=latest_data['Stress'],\n",
        "                       colorscale='RdYlGn_r',\n",
        "                       showscale=True),\n",
        "            text=latest_data['Node'],\n",
        "            name='Nodes'\n",
        "        ),\n",
        "        row=1, col=3\n",
        "    )\n",
        "    fig.update_layout(scene1 = dict(xaxis_title='Coherence', yaxis_title='Capacity', zaxis_title='Abstraction'))\n",
        "\n",
        "\n",
        "    # Panel 4: Coherence Entropy\n",
        "    entropy_values = []\n",
        "    for year in years:\n",
        "        year_data = df[df['Year'] == year]\n",
        "        coherence_vals = year_data['Coherence'].values\n",
        "        coherence_probs = coherence_vals / (np.sum(coherence_vals) + 1e-10) # Added small epsilon\n",
        "        entropy_val = entropy(coherence_probs) # Using scipy.stats.entropy\n",
        "        entropy_values.append(entropy_val)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=entropy_values, mode='lines+markers',\n",
        "                  name='Coherence Entropy', line=dict(color='#9333EA')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    fig.update_yaxes(title_text=\"Entropy\", row=2, col=1)\n",
        "\n",
        "\n",
        "    # Panel 5: Phase Space (Coherence vs Stress)\n",
        "    for node in df['Node'].unique():\n",
        "        node_data = df[df['Node'] == node]\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=node_data['Coherence'], y=node_data['Stress'],\n",
        "                      mode='lines', name=node, opacity=0.6),\n",
        "            row=2, col=2\n",
        "        )\n",
        "    fig.update_xaxes(title_text=\"Coherence\", row=2, col=2)\n",
        "    fig.update_yaxes(title_text=\"Stress\", row=2, col=2)\n",
        "\n",
        "\n",
        "    # Panel 6: Bond Matrix Heatmap\n",
        "    nodes = df['Node'].unique()\n",
        "    n_nodes = len(nodes)\n",
        "    # Simplified - using a random matrix as a placeholder for actual bond calculation\n",
        "    bond_matrix = np.random.rand(n_nodes, n_nodes)\n",
        "    np.fill_diagonal(bond_matrix, 0)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Heatmap(z=bond_matrix, x=nodes, y=nodes, colorscale='Viridis', name='Bond Strength'),\n",
        "        row=2, col=3\n",
        "    )\n",
        "\n",
        "\n",
        "    # Panel 7: Early Warning Signals (with secondary y-axis)\n",
        "    variance_rolling = pd.Series(health_by_year).rolling(window=3).std()\n",
        "    autocorrelation_rolling = pd.Series(health_by_year).rolling(window=3).apply(lambda x: pd.Series(x).autocorr(), raw=True)\n",
        "\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=health_by_year, name='System Health',\n",
        "                  line=dict(color='green')),\n",
        "        row=3, col=1, secondary_y=False\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=variance_rolling, name='Rolling Variance',\n",
        "                  line=dict(color='red', dash='dash')),\n",
        "        row=3, col=1, secondary_y=True\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=autocorrelation_rolling, name='Rolling Autocorrelation',\n",
        "                  line=dict(color='purple', dash='dot')),\n",
        "        row=3, col=1, secondary_y=True\n",
        "    )\n",
        "    fig.update_yaxes(title_text=\"System Health\", row=3, col=1, secondary_y=False)\n",
        "    fig.update_yaxes(title_text=\"Early Warning Signal Magnitude\", row=3, col=1, secondary_y=True)\n",
        "\n",
        "\n",
        "    # Panel 8: Capacity Evolution\n",
        "    # Select top 4 nodes based on latest capacity\n",
        "    latest_capacity = latest_data.set_index('Node')['Capacity'].sort_values(ascending=False)\n",
        "    top_nodes = latest_capacity.head(4).index.tolist()\n",
        "\n",
        "    for node in top_nodes:\n",
        "        node_data = df[df['Node'] == node]\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=node_data['Year'], y=node_data['Capacity'],\n",
        "                      mode='lines+markers', name=node),\n",
        "            row=3, col=2\n",
        "        )\n",
        "    fig.update_yaxes(title_text=\"Capacity\", row=3, col=2)\n",
        "\n",
        "\n",
        "    # Panel 9: Abstraction Dynamics\n",
        "    abstraction_mean = df.groupby('Year')['Abstraction'].mean()\n",
        "    abstraction_std = df.groupby('Year')['Abstraction'].std()\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=abstraction_mean, mode='lines',\n",
        "                  name='Mean Abstraction', line=dict(color='purple')),\n",
        "        row=3, col=3\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=abstraction_mean + abstraction_std,\n",
        "                  mode='lines', line=dict(color='purple', width=0),\n",
        "                  showlegend=False),\n",
        "        row=3, col=3\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=abstraction_mean - abstraction_std,\n",
        "                  mode='lines', line=dict(color='purple', width=0),\n",
        "                  fill='tonexty', fillcolor='rgba(147,51,234,0.2)',\n",
        "                  name='¬±1 SD'),\n",
        "        row=3, col=3\n",
        "    )\n",
        "    fig.update_yaxes(title_text=\"Abstraction\", row=3, col=3)\n",
        "\n",
        "\n",
        "    # Panel 10: Stress Propagation Network (using correlation heatmap)\n",
        "    stress_correlation = df.pivot_table(values='Stress', index='Year', columns='Node').corr()\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Heatmap(z=stress_correlation.values, x=nodes, y=nodes,\n",
        "                  colorscale='RdBu', zmid=0, name='Stress Correlation'),\n",
        "        row=4, col=1\n",
        "    )\n",
        "\n",
        "\n",
        "    # Panel 11: Legitimacy Index\n",
        "    legitimacy = []\n",
        "    for year in years:\n",
        "        year_data = df[df['Year'] == year]\n",
        "        # Recalculate health for legitimacy calculation\n",
        "        health = (year_data['Coherence'] * year_data['Capacity']).sum() / (1 + year_data['Stress'].abs().sum())\n",
        "        info = (year_data['Abstraction'] * year_data['Capacity']).sum() * 0.1\n",
        "        coherence_mean = year_data['Coherence'].mean()\n",
        "        legit = 0.4 * health + 0.3 * info + 0.3 * coherence_mean\n",
        "        legitimacy.append(legit)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=legitimacy, mode='lines+markers',\n",
        "                  name='Legitimacy', line=dict(color='#059669', width=2)),\n",
        "        row=4, col=2\n",
        "    )\n",
        "    fig.update_yaxes(title_text=\"Legitimacy Index\", row=4, col=2)\n",
        "\n",
        "\n",
        "    # Panel 12: Path Dependence (using cumulative coherence asymmetry)\n",
        "    coherence_std_by_year = df.groupby('Year')['Coherence'].std()\n",
        "    coherence_mean_by_year = df.groupby('Year')['Coherence'].mean()\n",
        "    coherence_asymmetry = coherence_std_by_year / (coherence_mean_by_year + 1e-10) # Added epsilon\n",
        "    path_dependence = np.cumprod(1 + coherence_asymmetry.fillna(0).values * 0.01) # Handle potential NaNs\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=path_dependence, mode='lines+markers',\n",
        "                  name='Path Dependence', line=dict(color='#7C3AED')),\n",
        "        row=4, col=3\n",
        "    )\n",
        "    fig.update_yaxes(title_text=\"Path Dependence (Cumulative Asymmetry)\", row=4, col=3)\n",
        "\n",
        "\n",
        "    # Panel 13: Comparative Resilience Score (Bar Chart for recent years)\n",
        "    resilience_scores = []\n",
        "    for year in years:\n",
        "        year_data = df[df['Year'] == year]\n",
        "        resilience = (year_data['Capacity'].mean() * year_data['Coherence'].mean()) / (year_data['Stress'].abs().mean() + 1e-10) # Added epsilon\n",
        "        resilience_scores.append(resilience)\n",
        "\n",
        "    # Show resilience for all years\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=years, y=resilience_scores, mode='lines+markers',\n",
        "               marker_color='teal', name='Resilience Score'),\n",
        "        row=5, col=1\n",
        "    )\n",
        "    fig.update_yaxes(title_text=\"Resilience Score\", row=5, col=1)\n",
        "\n",
        "\n",
        "    # Panel 14: System Status Indicator (Gauge)\n",
        "    current_health = health_by_year.iloc[-1] if not health_by_year.empty else 0\n",
        "    previous_health = health_by_year.iloc[-2] if len(health_by_year) > 1 else current_health\n",
        "\n",
        "    status_color = \"green\" if current_health > 3 else \"yellow\" if current_health > 1.5 else \"red\"\n",
        "    status_text = \"Stable\" if current_health > 3 else \"Warning\" if current_health > 1.5 else \"Critical\"\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Indicator(\n",
        "            mode=\"gauge+number+delta\",\n",
        "            value=current_health,\n",
        "            title={'text': f\"System Status: {status_text}\"},\n",
        "            delta={'reference': previous_health, 'relative': True, 'valueformat': \".2f\"}, # Added relative delta\n",
        "            gauge={'axis': {'range': [0, 5]}, # Adjusted range\n",
        "                   'bar': {'color': status_color},\n",
        "                   'steps': [\n",
        "                       {'range': [0, 1.5], 'color': \"red\"}, # Adjusted steps colors\n",
        "                       {'range': [1.5, 3], 'color': \"yellow\"},\n",
        "                       {'range': [3, 5], 'color': \"green\"}],\n",
        "                   'threshold': {'line': {'color': \"black\", 'width': 2}, # Adjusted threshold appearance\n",
        "                                'thickness': 0.75, 'value': 1.5}} # Set threshold at 1.5 for Critical\n",
        "        ),\n",
        "        row=5, col=2\n",
        "    )\n",
        "\n",
        "\n",
        "    # Panel 15: Future Projections (simplified)\n",
        "    if len(years) > 3:\n",
        "        # Simple linear projection\n",
        "        recent_health = health_by_year.iloc[-3:].values\n",
        "        x = np.arange(3)\n",
        "        z = np.polyfit(x, recent_health, 1)\n",
        "        future_years = list(range(years[-1] + 1, years[-1] + 6))\n",
        "        future_health = [np.polyval(z, i + 3) for i in range(5)]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=years, y=health_by_year, mode='lines+markers',\n",
        "                      name='Historical', line=dict(color='blue')),\n",
        "            row=5, col=3\n",
        "        )\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=future_years, y=future_health, mode='lines+markers',\n",
        "                      name='Projected', line=dict(color='orange', dash='dash')),\n",
        "            row=5, col=3\n",
        "        )\n",
        "    fig.update_xaxes(title_text=\"Year\", row=5, col=3)\n",
        "    fig.update_yaxes(title_text=\"System Health\", row=5, col=3)\n",
        "\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        height=2000, # Increased height for better spacing\n",
        "        width=1600,\n",
        "        title_text=f\"<b>{society} - {civ_analysis['type']}</b><br><sup>{civ_analysis['description']}</sup>\",\n",
        "        title_x=0.5,\n",
        "        showlegend=True,\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "\n",
        "\n",
        "    return fig, civ_analysis\n",
        "\n",
        "def generate_policy_insights(df, civ_analysis):\n",
        "    \"\"\"Generate evidence-based policy recommendations\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    # Latest year data\n",
        "    latest_year = df['Year'].max()\n",
        "    latest_data = df[df['Year'] == latest_year]\n",
        "\n",
        "    # Historical trends\n",
        "    years = sorted(df['Year'].unique())\n",
        "    if len(years) > 5:\n",
        "        recent_years = years[-5:]\n",
        "        recent_data = df[df['Year'].isin(recent_years)]\n",
        "\n",
        "        # Stress trend\n",
        "        stress_trend = recent_data.groupby('Year')['Stress'].mean()\n",
        "        if stress_trend.iloc[-1] > stress_trend.iloc[0] * 1.2:\n",
        "            insights.append({\n",
        "                'priority': 'HIGH',\n",
        "                'category': 'Stress Management',\n",
        "                'insight': 'Stress levels have increased by >20% in recent years',\n",
        "                'recommendation': 'Implement stress reduction measures: strengthen social safety nets, improve institutional capacity, enhance crisis response systems'\n",
        "            })\n",
        "\n",
        "        # Coherence trend\n",
        "        coherence_trend = recent_data.groupby('Year')['Coherence'].mean()\n",
        "        if coherence_trend.iloc[-1] < coherence_trend.iloc[0] * 0.8:\n",
        "            insights.append({\n",
        "                'priority': 'HIGH',\n",
        "                'category': 'Social Cohesion',\n",
        "                'insight': 'Coherence has declined by >20%, indicating growing fragmentation',\n",
        "                'recommendation': 'Foster inclusive dialogue, strengthen shared institutions, promote common narratives that unite rather than divide'\n",
        "            })\n",
        "\n",
        "        # Capacity analysis\n",
        "        capacity_variance = latest_data['Capacity'].std()\n",
        "        if capacity_variance > 2:\n",
        "            insights.append({\n",
        "                'priority': 'MEDIUM',\n",
        "                'category': 'Capacity Distribution',\n",
        "                'insight': 'High variance in capacity across nodes suggests inequality',\n",
        "                'recommendation': 'Redistribute resources to strengthen weaker nodes, particularly focusing on education and infrastructure'\n",
        "            })\n",
        "\n",
        "    # Node-specific insights\n",
        "    critical_nodes_stress = latest_data[latest_data['Stress'] > 7]['Node'].tolist()\n",
        "    if critical_nodes_stress:\n",
        "        insights.append({\n",
        "            'priority': 'HIGH',\n",
        "            'category': 'Critical Nodes (Stress)',\n",
        "            'insight': f\"Nodes under critical stress: {', '.join(critical_nodes_stress)}\",\n",
        "            'recommendation': 'Provide targeted support to these sectors to prevent cascade failures'\n",
        "        })\n",
        "\n",
        "    # Nodes with low capacity or coherence\n",
        "    low_capacity_nodes = latest_data[latest_data['Capacity'] < 3]['Node'].tolist()\n",
        "    low_coherence_nodes = latest_data[latest_data['Coherence'] < 3]['Node'].tolist()\n",
        "\n",
        "    if low_capacity_nodes:\n",
        "         insights.append({\n",
        "            'priority': 'MEDIUM',\n",
        "            'category': 'Critical Nodes (Capacity)',\n",
        "            'insight': f\"Nodes with low capacity: {', '.join(low_capacity_nodes)}\",\n",
        "            'recommendation': 'Invest in capacity building programs and resource allocation for these nodes.'\n",
        "        })\n",
        "\n",
        "    if low_coherence_nodes:\n",
        "         insights.append({\n",
        "            'priority': 'MEDIUM',\n",
        "            'category': 'Critical Nodes (Coherence)',\n",
        "            'insight': f\"Nodes with low coherence: {', '.join(low_coherence_nodes)}\",\n",
        "            'recommendation': 'Focus on improving communication, trust, and shared understanding within and between these nodes.'\n",
        "        })\n",
        "\n",
        "\n",
        "    # Abstraction gap\n",
        "    abstraction_gap = latest_data['Abstraction'].max() - latest_data['Abstraction'].min()\n",
        "    if abstraction_gap > 4:\n",
        "        insights.append({\n",
        "            'priority': 'MEDIUM',\n",
        "            'category': 'Innovation Gap',\n",
        "            'insight': 'Large abstraction gap indicates uneven technological/educational development',\n",
        "            'recommendation': 'Invest in knowledge transfer, education, and innovation infrastructure in lagging sectors'\n",
        "        })\n",
        "\n",
        "    # Civilization-type specific recommendations\n",
        "    if civ_analysis['type'] == 'Supergiant':\n",
        "        insights.append({\n",
        "            'priority': 'MEDIUM',\n",
        "            'category': 'Sustainability',\n",
        "            'insight': 'Supergiant civilizations risk burnout from high energy expenditure',\n",
        "            'recommendation': 'Build reserves, improve efficiency, and prepare for controlled deceleration'\n",
        "        })\n",
        "    elif civ_analysis['type'] == 'Fragile High-Stress':\n",
        "        insights.append({\n",
        "            'priority': 'CRITICAL',\n",
        "            'category': 'System Stability',\n",
        "            'insight': 'System operating at dangerous stress levels',\n",
        "            'recommendation': 'Immediate de-escalation needed: reduce external pressures, strengthen safety valves, improve adaptive capacity'\n",
        "        })\n",
        "\n",
        "    return insights\n",
        "\n",
        "def comparative_analysis(dataframes_dict):\n",
        "    \"\"\"Compare multiple civilizations\"\"\"\n",
        "    comparison_results = []\n",
        "\n",
        "    for name, df in dataframes_dict.items():\n",
        "        civ_analysis = analyze_civilization_type(df)\n",
        "\n",
        "        # Calculate key metrics\n",
        "        latest_year = df['Year'].max()\n",
        "        latest_data = df[df['Year'] == latest_year]\n",
        "\n",
        "        result = {\n",
        "            'Society': name,\n",
        "            'Type': civ_analysis['type'],\n",
        "            'Latest_Year': latest_year,\n",
        "            'System_Health': (latest_data['Coherence'] * latest_data['Capacity']).sum() / (1 + latest_data['Stress'].abs().sum()),\n",
        "            'Avg_Stress': latest_data['Stress'].abs().mean(),\n",
        "            'Avg_Coherence': latest_data['Coherence'].mean(),\n",
        "            'Avg_Capacity': latest_data['Capacity'].mean(),\n",
        "            'Avg_Abstraction': latest_data['Abstraction'].mean(),\n",
        "            'Resilience_Score': (latest_data['Capacity'].mean() * latest_data['Coherence'].mean()) / (latest_data['Stress'].abs().mean() + 1)\n",
        "        }\n",
        "        comparison_results.append(result)\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_results)\n",
        "\n",
        "    # Create comparison visualization\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('System Health Comparison', 'Stress vs Resilience',\n",
        "                       'Civilization Types', 'Capacity-Abstraction Space'),\n",
        "        specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "               [{\"type\": \"pie\"}, {\"type\": \"scatter\"}]]\n",
        "    )\n",
        "\n",
        "    # System Health Bar Chart\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=comparison_df['Society'], y=comparison_df['System_Health'],\n",
        "               marker_color='lightblue', name='System Health'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Stress vs Resilience Scatter\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=comparison_df['Avg_Stress'], y=comparison_df['Resilience_Score'],\n",
        "                  mode='markers+text', text=comparison_df['Society'],\n",
        "                  marker=dict(size=15), name='Societies'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # Civilization Types Pie Chart\n",
        "    type_counts = comparison_df['Type'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Pie(labels=type_counts.index, values=type_counts.values),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Capacity-Abstraction Space\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=comparison_df['Avg_Capacity'], y=comparison_df['Avg_Abstraction'],\n",
        "                  mode='markers+text', text=comparison_df['Society'],\n",
        "                  marker=dict(size=comparison_df['System_Health']*10,\n",
        "                            color=comparison_df['Avg_Stress'],\n",
        "                            colorscale='RdYlGn_r',\n",
        "                            showscale=True),\n",
        "                  name='Societies'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    fig.update_layout(height=800, width=1200, title_text=\"Comparative Civilizational Analysis\")\n",
        "\n",
        "    return comparison_df, fig\n",
        "\n",
        "# ============= MAIN EXECUTION =============\n",
        "\n",
        "def run_enhanced_cams_analysis():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üß† CAMS-CAN v4.0 - Enhanced Complex Adaptive Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        df = load_and_validate_data()\n",
        "        society_name = df['Society'].iloc[0]\n",
        "    except ValueError as e:\n",
        "        print(f\"‚ùå Data loading failed: {e}\")\n",
        "        print(\"Please ensure you upload a valid CSV file with the required columns.\")\n",
        "        return None, None, None, None # Return None to indicate failure\n",
        "\n",
        "\n",
        "    # Run dynamic simulation\n",
        "    print(\"\\nüîÑ Running dynamic simulation...\")\n",
        "    latest_year = df['Year'].max()\n",
        "    latest_data = df[df['Year'] == latest_year]\n",
        "\n",
        "    network = CAMSNetwork(latest_data.to_dict('records'))\n",
        "    simulation_history = network.simulate(steps=50)\n",
        "\n",
        "    # Create dashboard\n",
        "    print(\"üìä Creating advanced dashboard...\")\n",
        "    dashboard, civ_analysis = create_advanced_dashboard(df)\n",
        "\n",
        "    # Generate insights\n",
        "    print(\"\\nüí° Generating policy insights...\")\n",
        "    insights = generate_policy_insights(df, civ_analysis)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nüèõÔ∏è Civilization Type: {civ_analysis['type']}\")\n",
        "    print(f\"üìù {civ_analysis['description']}\")\n",
        "\n",
        "    print(\"\\nüìã Key Policy Insights:\")\n",
        "    # Display all insights\n",
        "    for insight in insights:\n",
        "        print(f\"\\n[{insight['priority']}] {insight['category']}\")\n",
        "        print(f\"  ‚Üí {insight['insight']}\")\n",
        "        print(f\"  ‚úì {insight['recommendation']}\")\n",
        "\n",
        "    # Calculate final metrics\n",
        "    # Use the network state after simulation\n",
        "    final_health = network.calculate_system_health()\n",
        "    final_entropy = network.calculate_coherence_entropy()\n",
        "    final_legitimacy = network.calculate_legitimacy()\n",
        "\n",
        "    print(\"\\nüìà System Metrics (after simulation):\") # Clarified metrics are after simulation\n",
        "    print(f\"  ‚Ä¢ System Health: {final_health:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Coherence Entropy: {final_entropy:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Legitimacy Index: {final_legitimacy:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Network Synchronization: {network.calculate_network_synchronization():.2f}\")\n",
        "    print(f\"  ‚Ä¢ Stress Variance: {network.calculate_stress_variance():.2f}\")\n",
        "\n",
        "\n",
        "    # Show and save dashboard\n",
        "    dashboard.show()\n",
        "\n",
        "    # Save results\n",
        "    dashboard.write_html(f\"{society_name}_cams_v4_dashboard.html\")\n",
        "    files.download(f\"{society_name}_cams_v4_dashboard.html\")\n",
        "\n",
        "    # Save insights to JSON\n",
        "    insights_data = {\n",
        "        'society': society_name,\n",
        "        'civilization_type': civ_analysis,\n",
        "        'insights': insights,\n",
        "        'metrics': {\n",
        "            'health': final_health,\n",
        "            'entropy': final_entropy,\n",
        "            'legitimacy': final_legitimacy,\n",
        "            'synchronization': network.calculate_network_synchronization(),\n",
        "            'stress_variance': network.calculate_stress_variance()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f\"{society_name}_insights.json\", 'w') as f:\n",
        "        json.dump(insights_data, f, indent=2, default=str)\n",
        "    files.download(f\"{society_name}_insights.json\")\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis complete! Files saved.\")\n",
        "\n",
        "    return df, network, dashboard, insights\n",
        "\n",
        "# Execute analysis\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df, network, dashboard, insights = run_enhanced_cams_analysis()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Please review the error message and your code/data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0d67626"
      },
      "source": [
        "# Task\n",
        "Modify the code to load the data from a CSV file located in Google Drive and save the generated dashboard HTML file to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7746b4b"
      },
      "source": [
        "## Mount google drive\n",
        "\n",
        "### Subtask:\n",
        "Add a cell to mount Google Drive to the Colab environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3674ef4d"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to add a cell to mount Google Drive so that files can be accessed from and saved to Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dacd4f8e",
        "outputId": "46cd4017-293f-40f2-b8dc-c78a58c37545"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09a0b7a1"
      },
      "source": [
        "## Modify data loading\n",
        "\n",
        "### Subtask:\n",
        "Update the `load_and_validate_data` function to load the CSV file from a specified path in Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91bd436b"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to modify the `load_and_validate_data` function to accept an optional file path for loading data from Google Drive. I will modify the function signature and add conditional logic to load the data using `pd.read_csv` if a file path is provided, while keeping the existing upload functionality as a fallback. I will also update the print statements accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a686786a"
      },
      "source": [
        "def load_and_validate_data(file_path=None):\n",
        "    \"\"\"Load CSV data with validation, from path or upload\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    if file_path:\n",
        "        print(f\"üìÅ Loading CAMS CSV file from path: {file_path}\")\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            print(f\"‚úÖ Loaded data from {file_path}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå File not found at {file_path}\")\n",
        "            raise ValueError(\"Specified file path not found\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading file from path: {e}\")\n",
        "            raise ValueError(f\"Error reading file: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"üìÅ Please upload your CAMS CSV file.\")\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                raise ValueError(\"No file uploaded\")\n",
        "            df = pd.read_csv(io.BytesIO(list(uploaded.values())[0]))\n",
        "            print(\"‚úÖ Loaded data from uploaded file\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå File upload failed: {e}\")\n",
        "            raise ValueError(f\"File upload failed: {e}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Validate columns\n",
        "    required = ['Society', 'Year', 'Node', 'Coherence', 'Capacity', 'Stress', 'Abstraction']\n",
        "    missing = [col for col in required if col not in df.columns]\n",
        "    if missing:\n",
        "        # Try alternative column names\n",
        "        if 'Nation' in df.columns and 'Society' not in df.columns:\n",
        "            df['Society'] = df['Nation']\n",
        "            missing.remove('Society') # Remove if fixed\n",
        "        if missing: # Check again after potential fix\n",
        "             raise ValueError(f\"Missing required columns after check: {', '.join(missing)}\")\n",
        "\n",
        "\n",
        "    # Clean data\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Calculate Node Value and Bond Strength if missing\n",
        "    if 'Node Value' not in df.columns:\n",
        "        df['Node Value'] = df['Coherence'] + df['Capacity'] - df['Stress'] + 0.5 * df['Abstraction']\n",
        "\n",
        "    if 'Bond Strength' not in df.columns:\n",
        "        # Simplified bond strength calculation\n",
        "        df['Bond Strength'] = ((df['Coherence'] + df['Capacity']) * 0.6 +\n",
        "                               df['Abstraction'] * 0.4) / (1 + np.abs(df['Stress']))\n",
        "\n",
        "    print(f\"üìä Years: {df['Year'].min()}-{df['Year'].max()}\")\n",
        "    print(f\"üîó Nodes: {len(df['Node'].unique())} unique nodes\")\n",
        "\n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5da9116d"
      },
      "source": [
        "## Modify plot saving\n",
        "\n",
        "### Subtask:\n",
        "Update the main execution block to save the generated dashboard HTML file to a specified path in Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "522dd51d"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the main execution function to save the dashboard HTML and insights JSON files to Google Drive paths and update the download paths accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38c36d16",
        "outputId": "da32ec13-a2db-418b-c8e0-56493eb4b5bb"
      },
      "source": [
        "def run_enhanced_cams_analysis(data_file_path=None, output_dir=\"/content/drive/My Drive/CAMS_Analysis\"):\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üß† CAMS-CAN v4.0 - Enhanced Complex Adaptive Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    import os\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"Output will be saved to: {output_dir}\")\n",
        "\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        df = load_and_validate_data(file_path=data_file_path)\n",
        "        society_name = df['Society'].iloc[0]\n",
        "    except ValueError as e:\n",
        "        print(f\"‚ùå Data loading failed: {e}\")\n",
        "        print(\"Please ensure you provide a valid file path or upload a valid CSV file with the required columns.\")\n",
        "        return None, None, None, None # Return None to indicate failure\n",
        "\n",
        "\n",
        "    # Run dynamic simulation\n",
        "    print(\"\\nüîÑ Running dynamic simulation...\")\n",
        "    latest_year = df['Year'].max()\n",
        "    latest_data = df[df['Year'] == latest_year]\n",
        "\n",
        "    # Check if latest_data is empty before proceeding\n",
        "    if latest_data.empty:\n",
        "        print(\"‚ùå Cannot run simulation: No data available for the latest year.\")\n",
        "        return df, None, None, None\n",
        "\n",
        "\n",
        "    network = CAMSNetwork(latest_data.to_dict('records'))\n",
        "    simulation_history = network.simulate(steps=50)\n",
        "\n",
        "    # Create dashboard\n",
        "    print(\"üìä Creating advanced dashboard...\")\n",
        "    dashboard, civ_analysis = create_advanced_dashboard(df)\n",
        "\n",
        "    # Generate insights\n",
        "    print(\"\\nüí° Generating policy insights...\")\n",
        "    insights = generate_policy_insights(df, civ_analysis)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nüèõÔ∏è Civilization Type: {civ_analysis['type']}\")\n",
        "    print(f\"üìù {civ_analysis['description']}\")\n",
        "\n",
        "    print(\"\\nüìã Key Policy Insights:\")\n",
        "    # Display all insights\n",
        "    for insight in insights:\n",
        "        print(f\"\\n[{insight['priority']}] {insight['category']}\")\n",
        "        print(f\"  ‚Üí {insight['insight']}\")\n",
        "        print(f\"  ‚úì {insight['recommendation']}\")\n",
        "\n",
        "    # Calculate final metrics\n",
        "    # Use the network state after simulation\n",
        "    final_health = network.calculate_system_health()\n",
        "    final_entropy = network.calculate_coherence_entropy()\n",
        "    final_legitimacy = network.calculate_legitimacy()\n",
        "\n",
        "    print(\"\\nüìà System Metrics (after simulation):\") # Clarified metrics are after simulation\n",
        "    print(f\"  ‚Ä¢ System Health: {final_health:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Coherence Entropy: {final_entropy:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Legitimacy Index: {final_legitimacy:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Network Synchronization: {network.calculate_network_synchronization():.2f}\")\n",
        "    print(f\"  ‚Ä¢ Stress Variance: {network.calculate_stress_variance():.2f}\")\n",
        "\n",
        "\n",
        "    # Show and save dashboard to Google Drive\n",
        "    dashboard.show()\n",
        "\n",
        "    dashboard_path = os.path.join(output_dir, f\"{society_name}_cams_v4_dashboard.html\")\n",
        "    dashboard.write_html(dashboard_path)\n",
        "    print(f\"‚úÖ Dashboard saved to {dashboard_path}\")\n",
        "    files.download(dashboard_path) # Download from the Google Drive path\n",
        "\n",
        "\n",
        "    # Save insights to JSON to Google Drive\n",
        "    insights_data = {\n",
        "        'society': society_name,\n",
        "        'civilization_type': civ_analysis,\n",
        "        'insights': insights,\n",
        "        'metrics': {\n",
        "            'health': final_health,\n",
        "            'entropy': final_entropy,\n",
        "            'legitimacy': final_legitimacy,\n",
        "            'synchronization': network.calculate_network_synchronization(),\n",
        "            'stress_variance': network.calculate_stress_variance()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    insights_path = os.path.join(output_dir, f\"{society_name}_insights.json\")\n",
        "    with open(insights_path, 'w') as f:\n",
        "        json.dump(insights_data, f, indent=2, default=str)\n",
        "    print(f\"‚úÖ Insights saved to {insights_path}\")\n",
        "    files.download(insights_path) # Download from the Google Drive path\n",
        "\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis complete! Files saved to Google Drive and downloaded.\")\n",
        "\n",
        "    return df, network, dashboard, insights\n",
        "\n",
        "# Execute analysis\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Example usage: specify the path to your CSV file in Google Drive\n",
        "        # Replace 'your_folder/your_data.csv' with the actual path\n",
        "        google_drive_data_path = '/content/drive/My Drive/CAMS_Analysis/sample_cams_data.csv'\n",
        "        google_drive_output_dir = '/content/drive/My Drive/CAMS_Analysis/Output'\n",
        "\n",
        "        df, network, dashboard, insights = run_enhanced_cams_analysis(\n",
        "            data_file_path=google_drive_data_path,\n",
        "            output_dir=google_drive_output_dir\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Please review the error message and your code/data.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üß† CAMS-CAN v4.0 - Enhanced Complex Adaptive Analysis\n",
            "============================================================\n",
            "Output will be saved to: /content/drive/My Drive/CAMS_Analysis/Output\n",
            "============================================================\n",
            "üìÅ Loading CAMS CSV file from path: /content/drive/My Drive/CAMS_Analysis/sample_cams_data.csv\n",
            "‚ùå File not found at /content/drive/My Drive/CAMS_Analysis/sample_cams_data.csv\n",
            "‚ùå Data loading failed: Specified file path not found\n",
            "Please ensure you provide a valid file path or upload a valid CSV file with the required columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53c224d1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The execution failed because the specified input data CSV file (`/content/drive/My Drive/CAMS_Analysis/sample_cams_data.csv`) was not found at the given path in Google Drive, preventing the analysis and file saving steps from completing.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Verify that the input CSV file exists at the specified Google Drive path (`/content/drive/My Drive/CAMS_Analysis/sample_cams_data.csv`) and that the path is correctly typed.\n",
        "*   Ensure that the Colab notebook has the necessary permissions to access files in the specified Google Drive location.\n"
      ]
    }
  ]
}